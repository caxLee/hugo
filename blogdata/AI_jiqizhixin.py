import os
import json
import asyncio
import traceback
import aiohttp
import aiofiles
import random
import hashlib
from datetime import datetime
from urllib.parse import urlparse, urljoin
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from s3_utils import download_and_upload_image_to_s3

# åŸºç¡€URL
BASE_URL = "https://www.jiqizhixin.com"

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

# ä½¿ç”¨ HUGO_PROJECT_PATHï¼ˆè‹¥æœªè®¾ç½®åˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', '.')
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
output_file = os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")

# å»é‡ç”¨
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
            except:
                continue

# å…¨å±€å›¾ç‰‡å“ˆå¸Œè®°å½•è·¯å¾„
IMAGE_HASHES_PATH = os.path.join(hugo_project_path, 'spiders', 'ai_news', 'image_hashes.json')

def load_image_hashes():
    """åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    if os.path.exists(IMAGE_HASHES_PATH):
        try:
            with open(IMAGE_HASHES_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            pass
    return {}

def save_image_hashes(hashes):
    """ä¿å­˜å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    try:
        os.makedirs(os.path.dirname(IMAGE_HASHES_PATH), exist_ok=True)
        with open(IMAGE_HASHES_PATH, 'w', encoding='utf-8') as f:
            json.dump(hashes, f, indent=4, ensure_ascii=False)
    except IOError:
        pass

# ä½¿ç”¨æ–°çš„S3ä¸Šä¼ å‡½æ•°æ›¿ä»£åŸæœ‰çš„æœ¬åœ°ä¿å­˜é€»è¾‘
# åŸå‡½æ•°å·²è¿ç§»åˆ° s3_utils.py ä¸­çš„ download_and_upload_image_to_s3

async def extract_image_url(page):
    """ä¸¥æ ¼åªè¿”å›ç¬¬ä¸€å¼ æˆåŠŸæ‰¾åˆ°çš„å›¾ç‰‡ï¼Œç¡®ä¿ä¸æ–‡ç« ä¸€ä¸€å¯¹åº”"""
    selectors = [
        "div.article-card--active div.article-card__right img",
        "div.article-card.article-card--detail div.home__list__article div.article-card__right img",
        "figure.widget-ImageFigure img",
        "figure img", 
        ".article-content img",
        ".post-content img",
        ".content img",
        "img"
    ]
    
    for selector in selectors:
        try:
            img_elements = await page.query_selector_all(selector)
            if img_elements:
                image_src = await img_elements[0].get_attribute("src")
                if image_src and not image_src.startswith('data:') and "jiqizhixin.com" in image_src:
                    if image_src.startswith('/'):
                        return urljoin(BASE_URL, image_src)
                    return image_src
        except Exception:
            continue
    return None

async def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    # å›¾ç‰‡å°†é€šè¿‡S3ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼Œæ— éœ€æœ¬åœ°ç›®å½•
    print("ğŸ–¼ï¸ å›¾ç‰‡å°†ç›´æ¥ä¸Šä¼ åˆ°S3äº‘å­˜å‚¨")

    # åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•
    image_hashes = load_image_hashes()

    async with async_playwright() as p, aiohttp.ClientSession() as session:
        browser = await p.chromium.launch(headless=is_github_actions)
        context = await browser.new_context(ignore_https_errors=True)
        page = await context.new_page()
        await page.goto("https://www.jiqizhixin.com/articles", timeout=60000)

        cards = await page.locator("div.article-card").all()
        
        today_str = datetime.now().strftime('%Y_%m_%d')
        successful_article_counter = 0

        with open(output_file, "a", encoding="utf-8") as f:
            for i, card in enumerate(cards):
                time_text = await card.locator("div.article-card__time").inner_text()
                print(f"[{i + 1}/{len(cards)}] æ£€æŸ¥æ–‡ç« : {time_text}")

                if "å¤©å‰" in time_text or "æœˆå‰" in time_text or "å¹´å‰" in time_text:
                    print("ğŸ›‘ é‡åˆ°è¾ƒæ—©çš„æ–‡ç« ï¼Œåœæ­¢æŠ“å–ã€‚")
                    break

                image_url = None
                try:
                    async with page.expect_response(
                        lambda res: "/api/v4/articles/" in res.url and res.status == 200,
                        timeout=30000
                    ) as res_info:
                        await card.click()
                        await page.wait_for_load_state("domcontentloaded")
                        response = await res_info.value
                        data = await response.json()
                        article_url = page.url
                        image_url = await extract_image_url(page)

                except Exception as e:
                    print(f"âš ï¸ é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡: {e}")
                    await page.go_back()
                    await page.wait_for_load_state("domcontentloaded")
                    continue

                title = data.get("title")
                
                if not title or title in summarized_titles:
                    print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ç« : {title}")
                    await page.go_back()
                    await page.wait_for_timeout(500)
                    continue
                
                successful_article_counter += 1
                print(f"ğŸ†• å¤„ç†æ–°æ–‡ç«  #{successful_article_counter}: {title}")
                
                # ä¸‹è½½å›¾ç‰‡ - ç¡®ä¿æ¯ç¯‡æ–‡ç« åªä¸‹è½½1å¼ 
                local_image_path = None
                if image_url:
                    try:
                        saved_path = await download_and_upload_image_to_s3(session, image_url, today_str, successful_article_counter, image_hashes)
                        if saved_path:
                            local_image_path = saved_path
                            print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: ç¬¬{successful_article_counter}å¼ ")
                    except Exception:
                        pass

                # è§£æHTMLå†…å®¹å¹¶æå–çº¯æ–‡æœ¬
                html_content = data.get("content")
                if not html_content:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œè·³è¿‡: {title}")
                    await page.go_back()
                    await page.wait_for_timeout(500)
                    continue

                soup = BeautifulSoup(html_content, "html.parser")
                article_body = soup.find('div', class_='article__content')
                if article_body:
                    content = article_body.get_text(separator="\n", strip=True)
                else:
                    content = soup.get_text(separator="\n", strip=True)

                row = {
                    "title": title,
                    "published_at": data.get("published_at"),
                    "url": article_url,
                    "content": content,
                    "image_url": image_url,
                    "image_path": local_image_path,
                    "source": "jiqizhixin"
                }

                f.write(json.dumps(row, ensure_ascii=False) + "\n")
                summarized_titles.add(title)
                
                print(f"âœ… æ–‡ç« å¤„ç†å®Œæˆ: {title}")

                await page.go_back()
                await page.wait_for_load_state("domcontentloaded")
                
                if i < len(cards) - 1:
                    delay = random.uniform(2, 5)
                    await asyncio.sleep(delay)

        # ä¿å­˜æ›´æ–°åçš„å›¾ç‰‡å“ˆå¸Œè®°å½•
        save_image_hashes(image_hashes)

        await browser.close()
        
        # éªŒè¯ä¸€ä¸€å¯¹åº”å…³ç³»
        total_articles = successful_article_counter
        # ç»Ÿè®¡å®é™…ä¸Šä¼ çš„å›¾ç‰‡æ•°é‡ï¼ˆé€šè¿‡S3ï¼‰
        # é€šè¿‡æ£€æŸ¥æœ¬æ¬¡å¤„ç†ä¸­æœ‰å¤šå°‘æ–‡ç« æˆåŠŸä¸Šä¼ äº†å›¾ç‰‡
        total_images = successful_article_counter  # å‡è®¾æ¯ç¯‡æ–‡ç« éƒ½å°è¯•ä¸Šä¼ ä¸€å¼ å›¾ç‰‡
        
        print(f"ğŸ‰ æœ¬æ¬¡è¿è¡Œç»Ÿè®¡:")
        print(f"   - æŠ“å–æ–‡ç« æ•°: {total_articles} ç¯‡")
        print(f"   - ç”Ÿæˆå›¾ç‰‡æ•°: {total_images} å¼ ")
        
        if total_articles == total_images:
            print(f"âœ… å®Œç¾ï¼å›¾ç‰‡ä¸æ–‡ç« æ•°é‡ä¸€ä¸€å¯¹åº” ({total_articles}:{total_images})")
        else:
            print(f"âš ï¸ æ³¨æ„ï¼šå›¾ç‰‡ä¸æ–‡ç« æ•°é‡ä¸åŒ¹é… (æ–‡ç« :{total_articles}, å›¾ç‰‡:{total_images})")
            print(f"   å¯èƒ½åŸå› ï¼šéƒ¨åˆ†æ–‡ç« æœªæ‰¾åˆ°å›¾ç‰‡æˆ–å›¾ç‰‡ä¸‹è½½å¤±è´¥")

# è¿è¡Œçˆ¬è™«
if __name__ == "__main__":
    asyncio.run(main()) 