import os
import json
import asyncio
import traceback
import aiohttp
import aiofiles
import random
import hashlib
from datetime import datetime
from urllib.parse import urlparse, urljoin
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# åŸºç¡€URL
BASE_URL = "https://www.jiqizhixin.com"

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

# ä½¿ç”¨ HUGO_PROJECT_PATHï¼ˆè‹¥æœªè®¾ç½®åˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ï¼‰
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', '.')
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
output_file = os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")

# å»é‡ç”¨
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
            except:
                continue

# å…¨å±€å›¾ç‰‡å“ˆå¸Œè®°å½•è·¯å¾„
IMAGE_HASHES_PATH = os.path.join(hugo_project_path, 'spiders', 'ai_news', 'image_hashes.json')

def load_image_hashes():
    """åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    if os.path.exists(IMAGE_HASHES_PATH):
        try:
            with open(IMAGE_HASHES_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            pass
    return {}

def save_image_hashes(hashes):
    """ä¿å­˜å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    try:
        os.makedirs(os.path.dirname(IMAGE_HASHES_PATH), exist_ok=True)
        with open(IMAGE_HASHES_PATH, 'w', encoding='utf-8') as f:
            json.dump(hashes, f, indent=4, ensure_ascii=False)
    except IOError:
        pass

async def download_and_process_image(session, url, date_str, index, image_hashes):
    """
    ä¸‹è½½å›¾ç‰‡,è®¡ç®—å†…å®¹å“ˆå¸Œ,æŸ¥é‡å¹¶æŒ‰æ—¥æœŸå’Œåºå·ä¿å­˜ã€‚
    è¿”å›å›¾ç‰‡åœ¨ä»“åº“ä¸­çš„ç›¸å¯¹è·¯å¾„,å¦‚æœå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    try:
        # æ£€æŸ¥URLæ˜¯å¦å·²ç»åœ¨å“ˆå¸Œè®°å½•ä¸­
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
        if url_hash in image_hashes:
            existing_path = image_hashes[url_hash]
            full_physical_path = os.path.join(hugo_project_path, 'static', existing_path)
            if os.path.exists(full_physical_path):
                return existing_path

        async with session.get(url) as response:
            if response.status != 200:
                return None
            
            image_data = await response.read()
            if not image_data:
                return None

            # è®¡ç®—å›¾ç‰‡å†…å®¹çš„å“ˆå¸Œå€¼
            image_hash = hashlib.sha256(image_data).hexdigest()

            # æ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å­˜åœ¨
            if image_hash in image_hashes:
                existing_path = image_hashes[image_hash]
                full_physical_path = os.path.join(hugo_project_path, 'static', existing_path)
                if os.path.exists(full_physical_path):
                    image_hashes[url_hash] = existing_path
                    return existing_path

            # åˆ›å»ºåŸºäºæ—¥æœŸçš„ç›®å½•
            date_folder = os.path.join(hugo_project_path, 'static', 'images', 'articles', date_str)
            os.makedirs(date_folder, exist_ok=True)

            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            file_ext = os.path.splitext(parsed_url.path)[1]
            if not file_ext or len(file_ext) > 5:
                content_type = response.headers.get('Content-Type', '')
                if 'jpeg' in content_type or 'jpg' in content_type: 
                    file_ext = '.jpg'
                elif 'png' in content_type: 
                    file_ext = '.png'
                elif 'gif' in content_type: 
                    file_ext = '.gif'
                elif 'webp' in content_type: 
                    file_ext = '.webp'
                else: 
                    file_ext = '.jpg'

            # æ„å»ºæ–°æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{index:03d}{file_ext}"
            hugo_relative_path = f"images/articles/{date_str}/{new_filename}"
            physical_save_path = os.path.join(date_folder, new_filename)

            async with aiofiles.open(physical_save_path, 'wb') as f:
                await f.write(image_data)

            # æ›´æ–°è®°å½•
            image_hashes[url_hash] = hugo_relative_path
            image_hashes[image_hash] = hugo_relative_path
            
            return hugo_relative_path
            
    except Exception:
        return None

async def extract_image_url(page):
    """ä¸¥æ ¼åªè¿”å›ç¬¬ä¸€å¼ æˆåŠŸæ‰¾åˆ°çš„å›¾ç‰‡ï¼Œç¡®ä¿ä¸æ–‡ç« ä¸€ä¸€å¯¹åº”"""
    selectors = [
        "div.article-card--active div.article-card__right img",
        "div.article-card.article-card--detail div.home__list__article div.article-card__right img",
        "figure.widget-ImageFigure img",
        "figure img", 
        ".article-content img",
        ".post-content img",
        ".content img",
        "img"
    ]
    
    for selector in selectors:
        try:
            img_elements = await page.query_selector_all(selector)
            if img_elements:
                image_src = await img_elements[0].get_attribute("src")
                if image_src and not image_src.startswith('data:') and "jiqizhixin.com" in image_src:
                    if image_src.startswith('/'):
                        return urljoin(BASE_URL, image_src)
                    return image_src
        except Exception:
            continue
    return None

async def main():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    image_save_dir = os.path.join(hugo_project_path, 'static', 'images', 'articles')
    os.makedirs(image_save_dir, exist_ok=True)

    # åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•
    image_hashes = load_image_hashes()

    async with async_playwright() as p, aiohttp.ClientSession() as session:
        browser = await p.chromium.launch(headless=is_github_actions)
        context = await browser.new_context(ignore_https_errors=True)
        page = await context.new_page()
        await page.goto("https://www.jiqizhixin.com/articles", timeout=60000)

        cards = await page.locator("div.article-card").all()
        
        today_str = datetime.now().strftime('%Y_%m_%d')
        successful_article_counter = 0

        with open(output_file, "a", encoding="utf-8") as f:
            for i, card in enumerate(cards):
                time_text = await card.locator("div.article-card__time").inner_text()
                print(f"[{i + 1}/{len(cards)}] æ£€æŸ¥æ–‡ç« : {time_text}")

                if "å¤©å‰" in time_text or "æœˆå‰" in time_text or "å¹´å‰" in time_text:
                    print("ğŸ›‘ é‡åˆ°è¾ƒæ—©çš„æ–‡ç« ï¼Œåœæ­¢æŠ“å–ã€‚")
                    break

                image_url = None
                try:
                    async with page.expect_response(
                        lambda res: "/api/v4/articles/" in res.url and res.status == 200,
                        timeout=30000
                    ) as res_info:
                        await card.click()
                        await page.wait_for_load_state("domcontentloaded")
                        response = await res_info.value
                        data = await response.json()
                        article_url = page.url
                        image_url = await extract_image_url(page)

                except Exception as e:
                    print(f"âš ï¸ é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡: {e}")
                    await page.go_back()
                    await page.wait_for_load_state("domcontentloaded")
                    continue

                title = data.get("title")
                
                if not title or title in summarized_titles:
                    print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ç« : {title}")
                    await page.go_back()
                    await page.wait_for_timeout(500)
                    continue
                
                successful_article_counter += 1
                print(f"ğŸ†• å¤„ç†æ–°æ–‡ç«  #{successful_article_counter}: {title}")
                
                # ä¸‹è½½å›¾ç‰‡ - ç¡®ä¿æ¯ç¯‡æ–‡ç« åªä¸‹è½½1å¼ 
                local_image_path = None
                if image_url:
                    try:
                        saved_path = await download_and_process_image(session, image_url, today_str, successful_article_counter, image_hashes)
                        if saved_path:
                            local_image_path = saved_path
                            print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: ç¬¬{successful_article_counter}å¼ ")
                    except Exception:
                        pass

                # è§£æHTMLå†…å®¹å¹¶æå–çº¯æ–‡æœ¬
                html_content = data.get("content")
                if not html_content:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œè·³è¿‡: {title}")
                    await page.go_back()
                    await page.wait_for_timeout(500)
                    continue

                soup = BeautifulSoup(html_content, "html.parser")
                article_body = soup.find('div', class_='article__content')
                if article_body:
                    content = article_body.get_text(separator="\n", strip=True)
                else:
                    content = soup.get_text(separator="\n", strip=True)

                row = {
                    "title": title,
                    "published_at": data.get("published_at"),
                    "url": article_url,
                    "content": content,
                    "image_url": image_url,
                    "image_path": local_image_path,
                    "source": "jiqizhixin"
                }

                f.write(json.dumps(row, ensure_ascii=False) + "\n")
                summarized_titles.add(title)
                
                print(f"âœ… æ–‡ç« å¤„ç†å®Œæˆ: {title}")

                await page.go_back()
                await page.wait_for_load_state("domcontentloaded")
                
                if i < len(cards) - 1:
                    delay = random.uniform(2, 5)
                    await asyncio.sleep(delay)

        # ä¿å­˜æ›´æ–°åçš„å›¾ç‰‡å“ˆå¸Œè®°å½•
        save_image_hashes(image_hashes)

        await browser.close()
        
        # éªŒè¯ä¸€ä¸€å¯¹åº”å…³ç³»
        total_articles = successful_article_counter
        total_images = 0
        
        # ç»Ÿè®¡å®é™…ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
        today_image_dir = os.path.join(hugo_project_path, 'static', 'images', 'articles', today_str)
        if os.path.exists(today_image_dir):
            total_images = len([f for f in os.listdir(today_image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))])
        
        print(f"ğŸ‰ æœ¬æ¬¡è¿è¡Œç»Ÿè®¡:")
        print(f"   - æŠ“å–æ–‡ç« æ•°: {total_articles} ç¯‡")
        print(f"   - ç”Ÿæˆå›¾ç‰‡æ•°: {total_images} å¼ ")
        
        if total_articles == total_images:
            print(f"âœ… å®Œç¾ï¼å›¾ç‰‡ä¸æ–‡ç« æ•°é‡ä¸€ä¸€å¯¹åº” ({total_articles}:{total_images})")
        else:
            print(f"âš ï¸ æ³¨æ„ï¼šå›¾ç‰‡ä¸æ–‡ç« æ•°é‡ä¸åŒ¹é… (æ–‡ç« :{total_articles}, å›¾ç‰‡:{total_images})")
            print(f"   å¯èƒ½åŸå› ï¼šéƒ¨åˆ†æ–‡ç« æœªæ‰¾åˆ°å›¾ç‰‡æˆ–å›¾ç‰‡ä¸‹è½½å¤±è´¥")

# è¿è¡Œçˆ¬è™«
if __name__ == "__main__":
    asyncio.run(main()) 