import os
import json
import hashlib
import textwrap  # å¯¼å…¥ textwrap
from openai import OpenAI
from tqdm import tqdm
from dotenv import load_dotenv
import sys

# æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­è¿è¡Œ
is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'


load_dotenv()


OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_API_BASE = os.getenv("OPENAI_API_BASE")

# ========== åˆå§‹åŒ– ==========
# ä½¿ç”¨ OpenAI v1.x+ å†…ç½®çš„é‡è¯•æœºåˆ¶
client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_API_BASE,
    timeout=60.0,  # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
    max_retries=5, # å†…ç½®çš„é‡è¯•æ¬¡æ•°
)


base = None  # SeaTable disabled
table_name = 'AIæ‘˜è¦'  # preserved as placeholder, not used

# --- ç¯å¢ƒè‡ªé€‚åº”çš„æ™ºèƒ½è·¯å¾„é…ç½® ---
hugo_project_path = ''
# é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨ GitHub Actions ç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS') == 'true':
    print("ğŸ¤– [AI_summary.py] åœ¨ GitHub Actions ä¸­è¿è¡Œ, å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ã€‚")
    hugo_project_path = os.getenv('HUGO_PROJECT_PATH')
    if not hugo_project_path:
        print("âŒ é”™è¯¯: åœ¨ GitHub Actions ç¯å¢ƒä¸­, ç¯å¢ƒå˜é‡ HUGO_PROJECT_PATH æœªè®¾ç½®ã€‚")
        sys.exit(1)
else:
    # å¦‚æœä¸åœ¨äº‘ç«¯ï¼Œåˆ™å‡å®šä¸ºæœ¬åœ°ç¯å¢ƒï¼Œè‡ªåŠ¨è®¡ç®—è·¯å¾„
    print("ğŸ’» [AI_summary.py] åœ¨æœ¬åœ°è¿è¡Œ, å°†è‡ªåŠ¨æ£€æµ‹é¡¹ç›®è·¯å¾„ã€‚")
    hugo_project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

print(f"âœ… [AI_summary.py] ä½¿ç”¨ Hugo é¡¹ç›®è·¯å¾„: {hugo_project_path}")
# --- è·¯å¾„é…ç½®ç»“æŸ ---

# æ–‡ä»¶è·¯å¾„ç°åœ¨å®Œå…¨åŸºäº hugo_project_path
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
input_files = [
    os.path.join(base_dir, "mit_news_articles.jsonl"),
    os.path.join(base_dir, "jiqizhixin_articles_summarized.jsonl")
]
output_file = os.path.join(base_dir, "summarized_articles.jsonl")
# åˆ é™¤ markdown_file å˜é‡ï¼Œä¸å†ç”ŸæˆMDæ–‡ä»¶

# ç”¨äºå»é‡çš„å†…å®¹å“ˆå¸Œé›†åˆ
content_hash_set = set()

# ç”¨äºè®¡ç®—å†…å®¹å“ˆå¸Œå€¼
def get_content_hash(content):
    return hashlib.md5(content.encode('utf-8')).hexdigest()

# åŠ è½½å·²æ€»ç»“çš„æ ‡é¢˜é›†åˆå’Œå†…å®¹å“ˆå¸Œé›†åˆ
summarized_titles = set()
if os.path.exists(output_file):
    with open(output_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                summarized_titles.add(data["title"])
                # åŒæ—¶è®°å½•å†…å®¹å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
                if "original_content" in data:
                    content_hash = get_content_hash(data["original_content"])
                    content_hash_set.add(content_hash)
            except:
                continue

# ä½¿ç”¨æœ€æ–°çš„OpenAI function callingæ ¼å¼
def call_openai_with_function_calling(content, title):
    """ä½¿ç”¨ multi-step agent è°ƒç”¨ OpenAI API ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦å’Œæ ‡ç­¾"""
    
    # é¢„å®šä¹‰çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆåŒ…å«"æœªè¯†åˆ«"ï¼‰
    predefined_tags = ["åŸºæ¨¡", "å¤šæ¨¡æ€", "Infra", "AI4S", "å…·èº«æ™ºèƒ½", "å‚ç›´å¤§æ¨¡å‹", "Agent", "èƒ½æ•ˆä¼˜åŒ–", "æœªè¯†åˆ«"]
    
    # Step 1: å†…å®¹åˆ†æå’Œå…³é”®ä¿¡æ¯æå–
    def extract_key_information(content, title):
        """ç¬¬ä¸€æ­¥ï¼šæå–å…³é”®ä¿¡æ¯"""
        tools_step1 = [
            {
                "type": "function",
                "function": {
                    "name": "extract_key_info",
                    "description": "ä»æ–‡ç« ä¸­æå–å…³é”®æŠ€æœ¯ä¿¡æ¯",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "main_technology": {
                                "type": "string",
                                "description": "æ–‡ç« æ¶‰åŠçš„ä¸»è¦æŠ€æœ¯æˆ–æ¦‚å¿µ"
                            },
                            "key_findings": {
                                "type": "string",
                                "description": "æ–‡ç« çš„æ ¸å¿ƒå‘ç°æˆ–æˆæœ"
                            },
                            "technical_impact": {
                                "type": "string",
                                "description": "æŠ€æœ¯å½±å“æˆ–åº”ç”¨ä»·å€¼"
                            },
                            "quantitative_data": {
                                "type": "string",
                                "description": "å…·ä½“çš„æ•°æ®ã€æŒ‡æ ‡æˆ–æ€§èƒ½æå‡ï¼ˆå¦‚æœ‰ï¼‰"
                            }
                        },
                        "required": ["main_technology", "key_findings", "technical_impact"]
                    }
                }
            }
        ]
        
        system_prompt_step1 = """ä½ æ˜¯ä¸€åæŠ€æœ¯ä¿¡æ¯åˆ†æä¸“å®¶ã€‚è¯·å®¢è§‚åœ°ä»æ–‡ç« ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œé¿å…ä½¿ç”¨ä»»ä½•ä¸»è§‚ä¿®é¥°è¯ã€‚
        
        è¦æ±‚ï¼š
        1. åªæå–å®¢è§‚äº‹å®ï¼Œä¸æ·»åŠ "é©å‘½æ€§"ã€"çªç ´æ€§"ç­‰ä¸»è§‚è¯„ä»·
        2. ä½¿ç”¨å‡†ç¡®çš„æŠ€æœ¯æœ¯è¯­
        3. å¦‚æœ‰å…·ä½“æ•°æ®æˆ–æŒ‡æ ‡ï¼Œè¯·å‡†ç¡®æå–
        4. ä¿æŒä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®¢è§‚æ€§"""
        
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt_step1},
                    {"role": "user", "content": f"æ ‡é¢˜ï¼š{title}\n\nå†…å®¹ï¼š\n{content}"}
                ],
                tools=tools_step1,
                tool_choice={"type": "function", "function": {"name": "extract_key_info"}},
                temperature=0.1
            )
            
            tool_calls = response.choices[0].message.tool_calls
            if tool_calls and tool_calls[0].function.name == "extract_key_info":
                args = json.loads(tool_calls[0].function.arguments)
                return args
            else:
                return None
        except Exception as e:
            print(f"âŒ å…³é”®ä¿¡æ¯æå–å¤±è´¥: {str(e)}")
            return None
    
    # Step 2: ç»“æ„åŒ–æ‘˜è¦ç”Ÿæˆ
    def generate_structured_summary(key_info):
        """ç¬¬äºŒæ­¥ï¼šåŸºäºæå–çš„ä¿¡æ¯ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
        tools_step2 = [
            {
                "type": "function",
                "function": {
                    "name": "generate_structured_summary",
                    "description": "ç”Ÿæˆç»“æ„åŒ–çš„ä¸‰å¥è¯æ‘˜è¦",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "sentence_1_what": {
                                "type": "string",
                                "description": "ç¬¬ä¸€å¥ï¼šæ˜¯ä»€ä¹ˆæŠ€æœ¯/ç ”ç©¶/äº§å“ï¼Œ10-15å­—"
                            },
                            "sentence_2_how": {
                                "type": "string",
                                "description": "ç¬¬äºŒå¥ï¼šå¦‚ä½•å®ç°/æ ¸å¿ƒæ–¹æ³•ï¼Œ15-20å­—"
                            },
                            "sentence_3_result": {
                                "type": "string",
                                "description": "ç¬¬ä¸‰å¥ï¼šç»“æœ/æ•ˆæœ/åº”ç”¨ï¼Œ15-20å­—"
                            }
                        },
                        "required": ["sentence_1_what", "sentence_2_how", "sentence_3_result"]
                    }
                }
            }
        ]
        
        system_prompt_step2 = """ä½ æ˜¯ä¸€åæŠ€æœ¯æ–‡æ¡£ç¼–è¾‘ä¸“å®¶ã€‚åŸºäºæå–çš„å…³é”®ä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„åŒ–çš„ä¸‰å¥è¯æ‘˜è¦ã€‚

        æ‘˜è¦ç»“æ„è¦æ±‚ï¼š
        1. ç¬¬ä¸€å¥ï¼šè¯´æ˜è¿™æ˜¯ä»€ä¹ˆæŠ€æœ¯/ç ”ç©¶/äº§å“ï¼ˆ10-15å­—ï¼‰
        2. ç¬¬äºŒå¥ï¼šè¯´æ˜å¦‚ä½•å®ç°æˆ–æ ¸å¿ƒæ–¹æ³•ï¼ˆ15-20å­—ï¼‰  
        3. ç¬¬ä¸‰å¥ï¼šè¯´æ˜ç»“æœã€æ•ˆæœæˆ–åº”ç”¨ä»·å€¼ï¼ˆ15-20å­—ï¼‰
        
        è¯­è¨€è¦æ±‚ï¼š
        1. ä½¿ç”¨å®¢è§‚ã€ç®€æ´çš„æè¿°
        2. é¿å…"é¦–æ¬¡"ã€"é©å‘½æ€§"ã€"çªç ´æ€§"ç­‰ä¸»è§‚ä¿®é¥°è¯
        3. ä½¿ç”¨å‡†ç¡®çš„æŠ€æœ¯æœ¯è¯­
        4. å¥å¼ç»Ÿä¸€ï¼Œè¡¨è¿°æ¸…æ™°
        5. æ€»å­—æ•°æ§åˆ¶åœ¨40-55å­—ä¹‹é—´"""
        
        key_info_text = f"""
        ä¸»è¦æŠ€æœ¯ï¼š{key_info.get('main_technology', '')}
        æ ¸å¿ƒå‘ç°ï¼š{key_info.get('key_findings', '')}
        æŠ€æœ¯å½±å“ï¼š{key_info.get('technical_impact', '')}
        é‡åŒ–æ•°æ®ï¼š{key_info.get('quantitative_data', '')}
        """
        
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt_step2},
                    {"role": "user", "content": key_info_text}
                ],
                tools=tools_step2,
                tool_choice={"type": "function", "function": {"name": "generate_structured_summary"}},
                temperature=0.1
            )
            
            tool_calls = response.choices[0].message.tool_calls
            if tool_calls and tool_calls[0].function.name == "generate_structured_summary":
                args = json.loads(tool_calls[0].function.arguments)
                # ç»„åˆä¸‰å¥è¯æˆä¸ºå®Œæ•´æ‘˜è¦
                summary = f"{args['sentence_1_what']}{args['sentence_2_how']}{args['sentence_3_result']}"
                return summary
            else:
                return ""
        except Exception as e:
            print(f"âŒ ç»“æ„åŒ–æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return ""
    
    # Step 3: æ ‡ç­¾åˆ†ç±»
    def classify_tags(key_info):
        """ç¬¬ä¸‰æ­¥ï¼šåŸºäºå…³é”®ä¿¡æ¯è¿›è¡Œæ ‡ç­¾åˆ†ç±»"""
        tools_step3 = [
            {
                "type": "function",
                "function": {
                    "name": "classify_article_tags",
                    "description": "åŸºäºå…³é”®ä¿¡æ¯å¯¹æ–‡ç« è¿›è¡Œæ ‡ç­¾åˆ†ç±»",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "tags": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": predefined_tags
                                },
                                "maxItems": 3,
                                "description": "å¿…é¡»ä¸”åªèƒ½ä»é¢„å®šä¹‰æ ‡ç­¾åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œæœ€å¤š3ä¸ªæ ‡ç­¾"
                            },
                            "reasoning": {
                                "type": "string",
                                "description": "é€‰æ‹©è¿™äº›æ ‡ç­¾çš„ç†ç”±"
                            }
                        },
                        "required": ["tags", "reasoning"]
                    }
                }
            }
        ]
        
        system_prompt_step3 = f"""ä½ æ˜¯ä¸€åAIæŠ€æœ¯åˆ†ç±»ä¸“å®¶ã€‚åŸºäºæå–çš„å…³é”®ä¿¡æ¯ï¼Œä»é¢„å®šä¹‰æ ‡ç­¾ä¸­é€‰æ‹©æœ€åˆé€‚çš„1-3ä¸ªæ ‡ç­¾ã€‚

        é¢„å®šä¹‰æ ‡ç­¾ï¼š{', '.join(predefined_tags)}
        
        åˆ†ç±»è§„åˆ™ï¼š
        1. ä¸¥æ ¼é™åˆ¶ï¼šåªèƒ½ä»é¢„å®šä¹‰æ ‡ç­¾ä¸­é€‰æ‹©
        2. æœ€å¤šé€‰æ‹©3ä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾
        3. å¦‚æœå†…å®¹ä¸æ‰€æœ‰æŠ€æœ¯æ ‡ç­¾éƒ½ä¸ç›¸å…³ï¼Œåˆ™åªè¿”å›["æœªè¯†åˆ«"]
        4. "æœªè¯†åˆ«"æ ‡ç­¾åªèƒ½å•ç‹¬å‡ºç°
        5. åŸºäºæŠ€æœ¯å†…å®¹çš„å®¢è§‚åŒ¹é…ï¼Œä¸åšä¸»è§‚æ¨æµ‹"""
        
        key_info_text = f"""
        ä¸»è¦æŠ€æœ¯ï¼š{key_info.get('main_technology', '')}
        æ ¸å¿ƒå‘ç°ï¼š{key_info.get('key_findings', '')}
        æŠ€æœ¯å½±å“ï¼š{key_info.get('technical_impact', '')}
        """
        
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt_step3},
                    {"role": "user", "content": key_info_text}
                ],
                tools=tools_step3,
                tool_choice={"type": "function", "function": {"name": "classify_article_tags"}},
                temperature=0.1
            )
            
            tool_calls = response.choices[0].message.tool_calls
            if tool_calls and tool_calls[0].function.name == "classify_article_tags":
                args = json.loads(tool_calls[0].function.arguments)
                tags = args.get("tags", [])
                reasoning = args.get("reasoning", "")
                
                # éªŒè¯æ ‡ç­¾çš„æœ‰æ•ˆæ€§
                valid_tags = []
                for tag in tags:
                    if tag in predefined_tags:
                        valid_tags.append(tag)
                    else:
                        print(f"âš ï¸ å¿½ç•¥æ— æ•ˆæ ‡ç­¾: {tag}")
                
                if not valid_tags:
                    print("â— æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾: 'æœªè¯†åˆ«'")
                    valid_tags = ["æœªè¯†åˆ«"]
                
                # ç¡®ä¿"æœªè¯†åˆ«"æ ‡ç­¾åªèƒ½å•ç‹¬å‡ºç°
                if "æœªè¯†åˆ«" in valid_tags and len(valid_tags) > 1:
                    print("âš ï¸ 'æœªè¯†åˆ«'æ ‡ç­¾ä¸èƒ½ä¸å…¶ä»–æ ‡ç­¾ä¸€èµ·ä½¿ç”¨ï¼Œåªä¿ç•™'æœªè¯†åˆ«'")
                    valid_tags = ["æœªè¯†åˆ«"]
                
                # é™åˆ¶æ ‡ç­¾æ•°é‡
                if len(valid_tags) > 3:
                    valid_tags = valid_tags[:3]
                    print(f"âš ï¸ æ ‡ç­¾æ•°é‡è¿‡å¤šï¼Œæˆªå–ä¸º: {valid_tags}")
                
                print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†ç±»ç†ç”±: {reasoning}")
                return valid_tags
            else:
                return ["æœªè¯†åˆ«"]
        except Exception as e:
            print(f"âŒ æ ‡ç­¾åˆ†ç±»å¤±è´¥: {str(e)}")
            return ["æœªè¯†åˆ«"]
    
    # æ‰§è¡Œå¤šæ­¥éª¤å¤„ç†æµç¨‹
    try:
        key_info = extract_key_information(content, title)
        if not key_info:
            print(f"âŒ å…³é”®ä¿¡æ¯æå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return "", ["æœªè¯†åˆ«"]
        
        summary = generate_structured_summary(key_info)
        if not summary:
            print(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥")
            return "", ["æœªè¯†åˆ«"]
        
        tags = classify_tags(key_info)
        
        print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ: {title[:30]}...")
        print(f"   æ‘˜è¦é•¿åº¦: {len(summary)} å­—")
        
        return summary, tags
        
    except Exception as e:
        print(f"âŒ Multi-step Agent å¤„ç†å¤±è´¥: {str(e)}")
        return "", ["æœªè¯†åˆ«"]

# å¤„ç†æ‰€æœ‰è¾“å…¥æ–‡ä»¶
articles = []
for input_file in input_files:
    if not os.path.exists(input_file):
        print(f"âš ï¸ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        continue
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line)
                if data.get("title") and data.get("content"):
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                    content_hash = get_content_hash(data["content"])
                    if content_hash in content_hash_set:
                        print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {data['title']}")
                        continue
                    
                    # å¦‚æœæ˜¯æ–°å†…å®¹ï¼Œåˆ™æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—ï¼ŒåŒæ—¶è®°å½•å“ˆå¸Œå€¼
                    articles.append(data)
                    content_hash_set.add(content_hash)
            except Exception as e:
                print(f"âš ï¸ è§£æJSONå¤±è´¥: {e}")

# ç”Ÿæˆæ‘˜è¦
print(f"å¼€å§‹ç”Ÿæˆæ‘˜è¦ï¼Œå…± {len(articles)} ç¯‡ï¼Œå·²æœ‰æ‘˜è¦ {len(summarized_titles)} ç¯‡")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(os.path.dirname(output_file), exist_ok=True)

# ä¿®æ”¹ï¼šåªå†™å…¥JSONLæ–‡ä»¶ï¼Œä¸å†ç”ŸæˆMDæ–‡ä»¶
with open(output_file, 'a', encoding='utf-8') as out_f:
    for article in tqdm(articles, desc="ğŸŒ æ­£åœ¨ç”Ÿæˆæ‘˜è¦"):
        title = article["title"]
        content = article["content"]
        url = article.get("url", "") # è·å–URL
        
        # å¦‚æœæ ‡é¢˜å·²å­˜åœ¨ï¼Œè·³è¿‡
        if title in summarized_titles:
            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ ‡é¢˜: {title}")
            continue

        # æ–°å¢: ä»åŸå§‹articleå­—å…¸ä¸­è·å– image_path
        image_path = article.get("image_path")

        try:
            # å¢åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
            print(f"\nğŸ¤– æ­£åœ¨ä¸ºæ–‡ç«  '{title}' å¯åŠ¨Multi-step Agent...")
            print(f"ğŸ“„ æ–‡ç« é•¿åº¦: {len(content)} å­—ç¬¦")
            # è°ƒç”¨ Multi-step Agent ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼ˆå¸¦é‡è¯•ï¼‰
            summary, tags = call_openai_with_function_calling(content, title)
            
            if not summary:
                print(f"âŒ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦ï¼Œè·³è¿‡: {title}")
                continue
                
            # ä¿å­˜æ‘˜è¦åˆ° jsonl æ–‡ä»¶
            article_data = {
                "title": title, 
                "summary": summary,
                "tags": tags,
                "url": url,
                "original_content": "",
                "image_path": image_path,
                "link": url
            }
            out_f.write(json.dumps(article_data, ensure_ascii=False) + "\n")
            out_f.flush()  # ç¡®ä¿ç«‹å³å†™å…¥ç£ç›˜
            
            # æ›´æ–°å·²å¤„ç†çš„æ ‡é¢˜é›†åˆ
            summarized_titles.add(title)

            print(f"âœ… æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜æ‘˜è¦: {title}")

        except Exception as e:
            print(f"\nâŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {title}\nåŸå› : {e}")

print("æ‰€æœ‰æ‘˜è¦å¤„ç†å®Œæˆã€‚")