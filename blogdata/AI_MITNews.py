import asyncio
import json
import random
from pathlib import Path
from playwright.async_api import async_playwright
import os
import aiohttp
import aiofiles
import uuid
from datetime import datetime
from urllib.parse import urlparse, urljoin
import hashlib
import logging
import traceback

BASE_URL = "https://news.mit.edu"
# ä½¿ç”¨ HUGO_PROJECT_PATH ä»¥ä¾¿åœ¨ GitHub Action ä¸­ä¹Ÿèƒ½è¿è¡Œ
hugo_project_path = os.getenv('HUGO_PROJECT_PATH', r'C:\Users\kongg\0')
base_dir = os.path.join(hugo_project_path, 'spiders', 'ai_news')
image_save_dir = os.path.join(hugo_project_path, 'static', 'images', 'articles')
SAVE_PATH = os.path.join(base_dir, "mit_news_articles.jsonl")
HEADLESS = os.environ.get('GITHUB_ACTIONS') == 'true'
# æ–°å¢è°ƒè¯•ç›®å½•
debug_dir = os.path.join(base_dir, "debug")
os.makedirs(debug_dir, exist_ok=True)

# å…¨å±€å›¾ç‰‡å“ˆå¸Œè®°å½•è·¯å¾„
IMAGE_HASHES_PATH = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'spiders', 'ai_news', 'image_hashes.json')

def load_image_hashes():
    """åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    if os.path.exists(IMAGE_HASHES_PATH):
        try:
            with open(IMAGE_HASHES_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"âš ï¸ åŠ è½½å›¾ç‰‡å“ˆå¸Œè®°å½•å¤±è´¥: {e}, å°†åˆ›å»ºä¸€ä¸ªæ–°çš„è®°å½•ã€‚")
            return {}
    return {}

def save_image_hashes(hashes):
    """ä¿å­˜å›¾ç‰‡å“ˆå¸Œè®°å½•"""
    try:
        os.makedirs(os.path.dirname(IMAGE_HASHES_PATH), exist_ok=True)
        with open(IMAGE_HASHES_PATH, 'w', encoding='utf-8') as f:
            json.dump(hashes, f, indent=4, ensure_ascii=False)
        print(f"âœ… å›¾ç‰‡å“ˆå¸Œè®°å½•å·²ä¿å­˜ï¼Œå…± {len(hashes)} æ¡è®°å½•")
    except IOError as e:
        print(f"âŒ ä¿å­˜å›¾ç‰‡å“ˆå¸Œè®°å½•å¤±è´¥: {e}")


def load_existing_urls(path):
    if not Path(path).exists():
        return set()
    with open(path, "r", encoding="utf-8") as f:
        return {json.loads(line)["url"] for line in f if line.strip()}


# ç½‘ç»œè¯·æ±‚ç›‘æ§å‡½æ•°
async def log_response(response):
    url = response.url
    status = response.status
    content_type = response.headers.get('content-type', '')
    
    # åªè®°å½•ä¸»è¦å“åº”
    if (status >= 400 or 'html' in content_type) and BASE_URL in url:
        print(f"ğŸŒ ç½‘ç»œå“åº”: URL={url}, çŠ¶æ€={status}, ç±»å‹={content_type}")
        
        if status >= 400:
            # ä¿å­˜é”™è¯¯å“åº”å†…å®¹ç”¨äºè°ƒè¯•
            try:
                body = await response.text()
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                debug_file = os.path.join(debug_dir, f"error_{status}_{timestamp}.html")
                async with aiofiles.open(debug_file, 'w', encoding='utf-8') as f:
                    await f.write(body)
                print(f"ğŸ“ å·²ä¿å­˜é”™è¯¯å“åº”åˆ° {debug_file}")
            except:
                print("âš ï¸ æ— æ³•ä¿å­˜é”™è¯¯å“åº”å†…å®¹")


async def download_and_process_image(session, url, date_str, index, image_hashes):
    """
    ä¸‹è½½å›¾ç‰‡,è®¡ç®—å“ˆå¸Œ,æŸ¥é‡å¹¶æŒ‰æ—¥æœŸå’Œåºå·ä¿å­˜ã€‚
    è¿”å›å›¾ç‰‡åœ¨ä»“åº“ä¸­çš„ç›¸å¯¹è·¯å¾„,å¦‚æœå¤±è´¥åˆ™è¿”å›Noneã€‚
    """
    try:
        # é¦–å…ˆæ£€æŸ¥URLæ˜¯å¦å·²ç»åœ¨å“ˆå¸Œè®°å½•ä¸­ï¼ˆåŸºäºURLçš„å¿«é€Ÿå»é‡ï¼‰
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
        if url_hash in image_hashes:
            existing_path = image_hashes[url_hash]
            print(f"ğŸ”„ å›¾ç‰‡URLå·²å­˜åœ¨ (URLå“ˆå¸Œ: {url_hash[:8]}...), ä½¿ç”¨ç°æœ‰è·¯å¾„: {existing_path}")
            # ç¡®è®¤æ–‡ä»¶ç‰©ç†å­˜åœ¨
            full_physical_path = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'static', existing_path)
            if os.path.exists(full_physical_path):
                return existing_path
            else:
                print(f"âš ï¸ æ–‡ä»¶è®°å½•å­˜åœ¨ä½†ç‰©ç†æ–‡ä»¶ä¸¢å¤±ï¼Œå°†é‡æ–°ä¸‹è½½: {existing_path}")

        async with session.get(url) as response:
            if response.status != 200:
                print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, URL: {url}")
                return None
            
            image_data = await response.read()
            if not image_data:
                print(f"âŒ ä¸‹è½½çš„å›¾ç‰‡æ•°æ®ä¸ºç©º, URL: {url}")
                return None

            # è®¡ç®—å›¾ç‰‡å†…å®¹çš„å“ˆå¸Œå€¼
            image_hash = hashlib.sha256(image_data).hexdigest()

            # æ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å­˜åœ¨äºè®°å½•ä¸­
            if image_hash in image_hashes:
                existing_path = image_hashes[image_hash]
                print(f"ğŸ”„ å›¾ç‰‡å†…å®¹å·²å­˜åœ¨ (å†…å®¹å“ˆå¸Œ: {image_hash[:8]}...), ä½¿ç”¨ç°æœ‰è·¯å¾„: {existing_path}")
                # ç¡®è®¤æ–‡ä»¶ç‰©ç†å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é‡æ–°ä¸‹è½½
                full_physical_path = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'static', existing_path)
                if os.path.exists(full_physical_path):
                    # åŒæ—¶æ›´æ–°URLå“ˆå¸Œè®°å½•
                    image_hashes[url_hash] = existing_path
                    return existing_path
                else:
                    print(f"âš ï¸ æ–‡ä»¶è®°å½•å­˜åœ¨ä½†ç‰©ç†æ–‡ä»¶ä¸¢å¤±ï¼Œå°†é‡æ–°ä¸‹è½½: {existing_path}")

            # åˆ›å»ºåŸºäºæ—¥æœŸçš„ç›®å½•
            date_folder = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'static', 'images', 'articles', date_str)
            os.makedirs(date_folder, exist_ok=True)

            # è·å–æ–‡ä»¶æ‰©å±•å
            parsed_url = urlparse(url)
            file_ext = os.path.splitext(parsed_url.path)[1]
            if not file_ext or len(file_ext) > 5: # åŸºæœ¬çš„æ‰©å±•åéªŒè¯
                # å°è¯•ä»Content-Typeè·å–
                content_type = response.headers.get('Content-Type', '')
                if 'jpeg' in content_type or 'jpg' in content_type:
                    file_ext = '.jpg'
                elif 'png' in content_type:
                    file_ext = '.png'
                elif 'gif' in content_type:
                    file_ext = '.gif'
                elif 'webp' in content_type:
                    file_ext = '.webp'
                else:
                    file_ext = '.jpg' # é»˜è®¤æ‰©å±•å

            # æ„å»ºæ–°æ–‡ä»¶åå’Œè·¯å¾„
            new_filename = f"{index:03d}{file_ext}" # 001.jpg, 002.png ...
            hugo_relative_path = f"images/articles/{date_str}/{new_filename}"
            physical_save_path = os.path.join(date_folder, new_filename)

            async with aiofiles.open(physical_save_path, 'wb') as f:
                await f.write(image_data)
            
            print(f"ğŸ–¼ï¸ æ–°å›¾ç‰‡å·²ä¿å­˜: {physical_save_path}")

            # æ›´æ–°è®°å½•ï¼šåŒæ—¶è®°å½•URLå“ˆå¸Œå’Œå†…å®¹å“ˆå¸Œ
            image_hashes[url_hash] = hugo_relative_path
            image_hashes[image_hash] = hugo_relative_path
            
            return hugo_relative_path

    except Exception as e:
        print(f"ğŸ’¥ ä¸‹è½½æˆ–å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


async def scrape_mit_news_articles(save_path):
    """ä¸»å‡½æ•°ï¼Œè´Ÿè´£æŠ“å–ã€å¤„ç†å’Œä¿å­˜MITæ–°é—»æ–‡ç« """
    # æ­¥éª¤1: åˆå§‹åŒ–è·¯å¾„å’Œæ•°æ®
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # ç¡®ä¿å›¾ç‰‡ä¿å­˜ç›®å½•å­˜åœ¨
    image_save_dir = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'static', 'images', 'articles')
    os.makedirs(image_save_dir, exist_ok=True)
    print(f"ğŸ–¼ï¸ å›¾ç‰‡å°†ç»Ÿä¸€ä¿å­˜åœ¨: {image_save_dir}")

    # åŠ è½½ç°æœ‰æ•°æ®ç”¨äºå»é‡
    existing_urls = load_existing_urls(save_path)
    print(f"ğŸ“‹ å·²åŠ è½½ {len(existing_urls)} ä¸ªç°æœ‰URLç”¨äºå»é‡")
    image_hashes = load_image_hashes()
    print(f"ğŸ–¼ï¸ å·²åŠ è½½ {len(image_hashes)} æ¡å›¾ç‰‡å“ˆå¸Œè®°å½•ç”¨äºæŸ¥é‡")

    browser = None
    try:
        # æ­¥éª¤2: å¯åŠ¨æµè§ˆå™¨å’Œä¼šè¯
        async with async_playwright() as p, aiohttp.ClientSession() as session:
            # åˆå§‹åŒ–æµè§ˆå™¨
            print(f"ğŸŒ ä½¿ç”¨{'æ— å¤´' if HEADLESS else 'å¯è§†åŒ–'}æµè§ˆå™¨æ¨¡å¼")
            browser = await p.chromium.launch(headless=HEADLESS)
            context = await browser.new_context(
                viewport={"width": 1280, "height": 800},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                ignore_https_errors=True
            )

            # è®¾ç½®æ›´å¤šè¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
            await context.set_extra_http_headers({
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Cache-Control': 'max-age=0',
                'Sec-Ch-Ua': '"Chromium";v="92", " Not A;Brand";v="99", "Google Chrome";v="92"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            })

            page = await context.new_page()
            
            # æ·»åŠ ç½‘ç»œç›‘æ§
            page.on("response", log_response)
            
            print("ğŸ”— æ­£åœ¨è®¿é—® MIT News é¦–é¡µ...")

            # å¢åŠ é‡è¯•é€»è¾‘ï¼Œåº”å¯¹ç½‘ç»œæ³¢åŠ¨
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    await page.goto(BASE_URL, timeout=60000)
                    print("âœ… æˆåŠŸè®¿é—® MIT News é¦–é¡µã€‚")
                    
                    # ä¿å­˜é¦–é¡µæˆªå›¾ç”¨äºè°ƒè¯•
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    screenshot_path = os.path.join(debug_dir, f"homepage_{timestamp}.png")
                    await page.screenshot(path=screenshot_path)
                    print(f"ğŸ“¸ å·²ä¿å­˜é¦–é¡µæˆªå›¾åˆ° {screenshot_path}")
                    
                    break  # æˆåŠŸï¼Œåˆ™è·³å‡ºå¾ªç¯
                except Exception as e:
                    print(f"ğŸ•’ è®¿é—®è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})ï¼Œæ­£åœ¨é‡è¯•...")
                    if attempt == max_retries - 1:
                        print(f"âŒ è®¿é—® MIT News å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        await browser.close()
                        return  # é€€å‡ºå‡½æ•°

            print("ğŸ” æ­£åœ¨æå–æ–°é—»æ ‡é¢˜å’Œé“¾æ¥...")
            # ç­‰å¾…ä¸€ä¸‹ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_load_state("networkidle", timeout=30000)
            
            # å°è¯•å¤šä¸ªé€‰æ‹©å™¨æŸ¥æ‰¾é“¾æ¥
            link_selectors = [
                "a.front-page--news-article--teaser--title--link",
                ".front-page--news-article--teaser a[href]",
                ".front-page--section--news-articles a[href]"
            ]
            
            all_articles = []
            for selector in link_selectors:
                try:
                    print(f"ğŸ” å°è¯•ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æŸ¥æ‰¾é“¾æ¥...")
                    found_links = await page.query_selector_all(selector)
                    if found_links and len(found_links) > 0:
                        for link in found_links:
                            href = await link.get_attribute("href")
                            # å°è¯•å¤šç§æ–¹å¼è·å–æ ‡é¢˜
                            title = ""
                            title_span = await link.query_selector("span")
                            if title_span:
                                title = await title_span.inner_text()
                            
                            # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡è·å–åˆ°æ ‡é¢˜ï¼Œå°è¯•ç›´æ¥è·å–é“¾æ¥æ–‡æœ¬
                            if not title:
                                title = await link.inner_text()
                            
                            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ ‡é¢˜ï¼Œå°è¯•è·å–å…¶ä»–å¯èƒ½åŒ…å«æ ‡é¢˜çš„å…ƒç´ 
                            if not title:
                                parent = await link.query_selector("xpath=..")
                                if parent:
                                    title_elem = await parent.query_selector("h3, h4, .title")
                                    if title_elem:
                                        title = await title_elem.inner_text()

                            if not href or not title.strip():
                                print(f"â­ï¸ è·³è¿‡æ— æ ‡é¢˜æˆ–æ— é“¾æ¥çš„é¡¹: {href}")
                                continue  # è·³è¿‡æ— æ ‡é¢˜çš„

                            # å¤„ç†ç›¸å¯¹URL
                            if href.startswith('/'):
                                full_url = BASE_URL + href
                            else:
                                full_url = href
                            
                            if full_url in existing_urls:
                                print(f"â­ï¸ å·²æŠ“å–ï¼Œè·³è¿‡ï¼š{full_url}")
                                continue
                            
                            all_articles.append((title.strip(), full_url))
                
                        if all_articles:
                            print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(all_articles)} ç¯‡æ–‡ç« é“¾æ¥ã€‚")
                            break # æ‰¾åˆ°é“¾æ¥åå°±è·³å‡ºå¾ªç¯
                        else:
                            print(f"  âŒ é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« é“¾æ¥ã€‚")
                except Exception as e:
                    print(f"âš ï¸ é€‰æ‹©å™¨ '{selector}' æŸ¥æ‰¾å¤±è´¥: {e}")
            
            if not all_articles:
                print("âŒ ä½¿ç”¨æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªèƒ½æå–åˆ°ä»»ä½•æ–‡ç« é“¾æ¥ã€‚")
                await browser.close()
                return

            print(f"\næ€»å…±æ‰¾åˆ° {len(all_articles)} ç¯‡ä¸é‡å¤çš„æ–‡ç« å¾…å¤„ç†ã€‚\n")

            new_articles = []
            today_str = datetime.now().strftime('%Y_%m_%d') # å½“å¤©æ—¥æœŸå­—ç¬¦ä¸²
            successful_article_counter = 0 # æˆåŠŸå¤„ç†çš„æ–‡ç« è®¡æ•°å™¨

            with open(save_path, "a", encoding="utf-8") as f:
                for i, (title, article_url) in enumerate(all_articles):
                    print(f"\n--- [{i+1}/{len(all_articles)}] æ­£åœ¨å¤„ç†: {title} ---")
                    print(f"URL: {article_url}")

                    # è®¿é—®æ–‡ç« é¡µé¢
                    try:
                        article_page = await context.new_page()
                        # æ·»åŠ ç½‘ç»œç›‘æ§
                        article_page.on("response", log_response)
                        
                        print(f"ğŸ”— å¼€å§‹è®¿é—®æ–‡ç« é¡µé¢...")
                        await article_page.goto(article_url, timeout=60000)
                        print(f"âœ… æˆåŠŸæ‰“å¼€æ–‡ç« é¡µé¢")
                        
                        # ä¿å­˜æ–‡ç« é¡µé¢æˆªå›¾ç”¨äºè°ƒè¯•
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        screenshot_path = os.path.join(debug_dir, f"article_{timestamp}.png")
                        await article_page.screenshot(path=screenshot_path)
                        print(f"ğŸ“¸ å·²ä¿å­˜æ–‡ç« é¡µé¢æˆªå›¾åˆ° {screenshot_path}")
                        
                        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                        await article_page.wait_for_load_state("networkidle", timeout=30000)
                        
                        # å°è¯•å¤šä¸ªé€‰æ‹©å™¨æŸ¥æ‰¾æ–‡ç« ä¸»ä½“
                        article_body_selector = None
                        article_selectors = [
                            "article.page--article--body", 
                            ".article-content", 
                            "main article", 
                            ".page-content", 
                            ".article", 
                            "main .content",
                            "body" # æœ€åçš„åå¤‡é€‰æ‹©å™¨
                        ]
                        
                        print("ğŸ” å¼€å§‹æŸ¥æ‰¾æ–‡ç« ä¸»ä½“å…ƒç´ ...")
                        for selector in article_selectors:
                            try:
                                print(f"  å°è¯•é€‰æ‹©å™¨: {selector}")
                                await article_page.wait_for_selector(selector, timeout=5000)
                                print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°æ–‡ç« ä¸»ä½“")
                                article_body_selector = selector
                                break
                            except Exception as e:
                                print(f"  âŒ é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°åŒ¹é…å…ƒç´ ")
                        
                        if not article_body_selector:
                            print("âŒ ä½¿ç”¨æ‰€æœ‰é€‰æ‹©å™¨éƒ½æ— æ³•æ‰¾åˆ°æ–‡ç« ä¸»ä½“ï¼Œä¿å­˜é¡µé¢å†…å®¹ç”¨äºè°ƒè¯•")
                            html_content = await article_page.content()
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            debug_file = os.path.join(debug_dir, f"no_article_body_{timestamp}.html")
                            async with aiofiles.open(debug_file, 'w', encoding='utf-8') as f:
                                await f.write(html_content)
                            print(f"ğŸ“ å·²ä¿å­˜é¡µé¢å†…å®¹åˆ° {debug_file}")
                            await article_page.close()
                            continue

                        # Extract image URL - ä¸¥æ ¼åªè·å–ç¬¬ä¸€å¼ å›¾ç‰‡
                        image_url = None
                        try:
                            selectors = [
                                "figure picture img",
                                "figure img",
                                "article img",
                                f"{article_body_selector} img",
                                ".page--article--body img",
                                ".media-image img",
                                "img.featured-image",
                                "img"
                            ]
                            
                            for selector in selectors:
                                try:
                                    img_elements = await article_page.query_selector_all(selector)
                                    if img_elements and len(img_elements) > 0:
                                        image_src = await img_elements[0].get_attribute("src")
                                        if image_src:
                                            if image_src.startswith('/'):
                                                image_url = urljoin(BASE_URL, image_src)
                                            else:
                                                image_url = image_src
                                            break  # æ‰¾åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡åç«‹å³åœæ­¢
                                except Exception:
                                    continue
                        except Exception:
                            pass

                        # Get article content
                        try:
                            selectors = [
                                "div.paragraph--type--content-block-text p", 
                                f"{article_body_selector} p", 
                                "article p", 
                                ".page-content p", 
                                ".article-content p",
                                "p"
                            ]
                            
                            paragraphs = []
                            for selector in selectors:
                                try:
                                    paragraph_elements = await article_page.query_selector_all(selector)
                                    if paragraph_elements and len(paragraph_elements) > 0:
                                        paragraphs = []
                                        for p in paragraph_elements:
                                            text = await p.inner_text()
                                            if text.strip():
                                                paragraphs.append(text.strip())
                                        if paragraphs:
                                            break
                                except Exception:
                                    continue
                            
                            # If still no paragraphs, get the whole article text as fallback
                            if not paragraphs:
                                try:
                                    article_body = await article_page.locator(article_body_selector).inner_text()
                                    if article_body:
                                        paragraphs = [article_body]
                                    else:
                                        paragraphs = ["[å†…å®¹æå–å¤±è´¥]"]
                                except Exception:
                                    paragraphs = ["[å†…å®¹æå–å¤±è´¥]"]
                            
                            article_text = "\n\n".join(paragraphs)
                        except Exception:
                            article_text = "[å†…å®¹æå–å¤±è´¥]"

                        # æ–‡ç« å†…å®¹æå–æˆåŠŸåï¼Œæ‰è¿›è¡Œå›¾ç‰‡ä¸‹è½½ï¼Œç¡®ä¿ä¸€ä¸€å¯¹åº”
                        local_image_path = None
                        if image_url and article_text != "[å†…å®¹æå–å¤±è´¥]":
                            try:
                                saved_path = await download_and_process_image(session, image_url, today_str, successful_article_counter + 1, image_hashes)
                                if saved_path:
                                    local_image_path = saved_path
                                    print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: ç¬¬{successful_article_counter + 1}å¼ ")
                            except Exception:
                                pass

                        # å°†æ–‡ç« æ•°æ®å†™å…¥æ–‡ä»¶
                        article_data = {
                            "title": title.strip(),
                            "content": article_text,
                            "url": article_url,
                            "image_path": local_image_path,
                            "source": "MIT News"
                        }
                        
                        f.write(json.dumps(article_data, ensure_ascii=False) + "\n")
                        new_articles.append(article_data)
                        existing_urls.add(article_url) # æ›´æ–°å·²å¤„ç†URLé›†åˆ
                        successful_article_counter += 1 # åªæœ‰æˆåŠŸå¤„ç†æ–‡ç« æ‰å¢åŠ è®¡æ•°å™¨
                        print(f"âœ… å·²æŠ“å–å¹¶ä¿å­˜æ–‡ç« æ•°æ®: {title} (ç¬¬ {successful_article_counter} ç¯‡)")

                    except Exception as e:
                        print(f"âŒ å¤„ç†æ–‡ç« é¡µé¢å¤±è´¥: {article_url}, é”™è¯¯: {e}")
                    
                    # éšæœºå»¶è¿Ÿ
                    if i < len(all_articles) - 1:
                        delay = random.uniform(1, 3)
                        print(f"--- ç­‰å¾… {delay:.2f} ç§’åç»§ç»­ ---\n")
                        await article_page.wait_for_timeout(delay * 1000)

        # æ­¥éª¤4: æ¸…ç†å’Œä¿å­˜
    finally:
        if browser:
            await browser.close()
        
        # ä¿å­˜æ›´æ–°åçš„å›¾ç‰‡å“ˆå¸Œè®°å½•
        save_image_hashes(image_hashes)
        print("âœ… å›¾ç‰‡å“ˆå¸Œè®°å½•å·²æ›´æ–°å¹¶ä¿å­˜ã€‚")
        
        # éªŒè¯ä¸€ä¸€å¯¹åº”å…³ç³»
        total_articles = len(new_articles)
        total_images = 0
        
        # ç»Ÿè®¡å®é™…ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
        today_image_dir = os.path.join(os.getenv('HUGO_PROJECT_PATH', '.'), 'static', 'images', 'articles', today_str)
        if os.path.exists(today_image_dir):
            # åªç»Ÿè®¡ä»Šå¤©æ–°ç”Ÿæˆçš„å›¾ç‰‡
            image_files = [f for f in os.listdir(today_image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))]
            total_images = len(image_files)
        
        print(f"ğŸ‰ æœ¬æ¬¡è¿è¡Œç»Ÿè®¡:")
        print(f"   - æŠ“å–æ–‡ç« æ•°: {total_articles} ç¯‡")
        print(f"   - ç”Ÿæˆå›¾ç‰‡æ•°: {total_images} å¼ ")
        
        if total_articles == total_images:
            print(f"âœ… å®Œç¾ï¼å›¾ç‰‡ä¸æ–‡ç« æ•°é‡ä¸€ä¸€å¯¹åº” ({total_articles}:{total_images})")
        elif total_images < total_articles:
            print(f"âš ï¸ æ³¨æ„ï¼šå›¾ç‰‡æ•°é‡å°‘äºæ–‡ç« æ•°é‡ (æ–‡ç« :{total_articles}, å›¾ç‰‡:{total_images})")
            print(f"   åŸå› ï¼š{total_articles - total_images} ç¯‡æ–‡ç« æœªæ‰¾åˆ°å›¾ç‰‡æˆ–å›¾ç‰‡ä¸‹è½½å¤±è´¥")
        else:
            print(f"âš ï¸ å¼‚å¸¸ï¼šå›¾ç‰‡æ•°é‡è¶…è¿‡æ–‡ç« æ•°é‡ (æ–‡ç« :{total_articles}, å›¾ç‰‡:{total_images})")
            print(f"   è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œè¯·æ£€æŸ¥ä»£ç é€»è¾‘")


if __name__ == "__main__":
    print(f"ğŸš€ MIT News çˆ¬è™«å¼€å§‹è¿è¡Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    asyncio.run(scrape_mit_news_articles(SAVE_PATH))
    print(f"ğŸ MIT News çˆ¬è™«è¿è¡Œç»“æŸ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
