import os
import hashlib
import aioboto3
from urllib.parse import urlparse
from dotenv import load_dotenv

load_dotenv()

# Cloudflare R2 é…ç½®
R2_ACCESS_KEY = os.getenv('R2_ACCESS_KEY', '3926e0aecd942e9ea45667eaff4095f6')
R2_SECRET_KEY = os.getenv('R2_SECRET_KEY', 'eb7520fc3e66534dc80f39beb7bc08153bc79bd2272f58a799e0ec0c26257256')
R2_ENDPOINT = os.getenv('R2_ENDPOINT', 'https://88a983a33ad1822518ad743ad7d98dcb.r2.cloudflarestorage.com')
R2_BUCKET = 'ainews'  # å›ºå®šçš„bucketåç§°
R2_PUBLIC_URL = os.getenv('R2_PUBLIC_URL', 'https://88a983a33ad1822518ad743ad7d98dcb.r2.cloudflarestorage.com/ainews')

class S3ImageUploader:
    def __init__(self):
        self.session = None
        self.s3_client = None
    
    async def __aenter__(self):
        self.session = aioboto3.Session(
            aws_access_key_id=R2_ACCESS_KEY,
            aws_secret_access_key=R2_SECRET_KEY
        )
        self.s3_client = await self.session.client(
            's3',
            endpoint_url=R2_ENDPOINT,
            region_name='auto'  # Cloudflare R2 ä½¿ç”¨ 'auto'
        ).__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.s3_client:
            await self.s3_client.__aexit__(exc_type, exc_val, exc_tb)
    
    async def upload_image(self, image_data, file_path, content_type=None):
        """
        ä¸Šä¼ å›¾ç‰‡åˆ°S3/R2
        
        Args:
            image_data: å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
            file_path: S3ä¸­çš„æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: images/articles/2024-09-10/001.jpg)
            content_type: MIMEç±»å‹ (å¯é€‰)
        
        Returns:
            å…¬å¼€è®¿é—®çš„URLå­—ç¬¦ä¸²ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šcontent_typeï¼Œæ ¹æ®æ–‡ä»¶æ‰©å±•åæ¨æ–­
            if not content_type:
                ext = os.path.splitext(file_path)[1].lower()
                content_type_map = {
                    '.jpg': 'image/jpeg',
                    '.jpeg': 'image/jpeg',
                    '.png': 'image/png',
                    '.gif': 'image/gif',
                    '.webp': 'image/webp'
                }
                content_type = content_type_map.get(ext, 'image/jpeg')
            
            # ä¸Šä¼ åˆ°S3/R2
            await self.s3_client.put_object(
                Bucket=R2_BUCKET,
                Key=file_path,
                Body=image_data,
                ContentType=content_type,
                CacheControl='public, max-age=31536000'  # ç¼“å­˜1å¹´
            )
            
            # è¿”å›å…¬å¼€è®¿é—®URL
            public_url = f"{R2_PUBLIC_URL}/{file_path}"
            return public_url
            
        except Exception as e:
            print(f"âŒ S3ä¸Šä¼ å¤±è´¥: {str(e)}")
            return None
    
    async def check_image_exists(self, file_path):
        """
        æ£€æŸ¥S3ä¸­æ˜¯å¦å·²å­˜åœ¨æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶
        
        Args:
            file_path: S3ä¸­çš„æ–‡ä»¶è·¯å¾„
        
        Returns:
            å¦‚æœå­˜åœ¨è¿”å›å…¬å¼€URLï¼Œå¦åˆ™è¿”å›None
        """
        try:
            await self.s3_client.head_object(Bucket=R2_BUCKET, Key=file_path)
            return f"{R2_PUBLIC_URL}/{file_path}"
        except:
            return None

def get_file_extension_from_url_or_content_type(url, content_type=''):
    """ä»URLæˆ–Content-Typeè·å–æ–‡ä»¶æ‰©å±•å"""
    # å…ˆå°è¯•ä»URLè·å–
    parsed_url = urlparse(url)
    file_ext = os.path.splitext(parsed_url.path)[1]
    
    if file_ext and len(file_ext) <= 5:
        return file_ext
    
    # å¦‚æœURLæ²¡æœ‰æœ‰æ•ˆæ‰©å±•åï¼Œä»Content-Typeæ¨æ–­
    if 'jpeg' in content_type or 'jpg' in content_type: 
        return '.jpg'
    elif 'png' in content_type: 
        return '.png'
    elif 'gif' in content_type: 
        return '.gif'
    elif 'webp' in content_type: 
        return '.webp'
    else: 
        return '.jpg'  # é»˜è®¤ä½¿ç”¨jpg

async def download_and_upload_image_to_s3(session, url, date_str, index, image_hashes):
    """
    ä¸‹è½½å›¾ç‰‡å¹¶ä¸Šä¼ åˆ°S3ï¼Œæ”¯æŒå»é‡
    
    Args:
        session: aiohttpå®¢æˆ·ç«¯session
        url: å›¾ç‰‡URL
        date_str: æ—¥æœŸå­—ç¬¦ä¸² (ä¾‹å¦‚: 2024-09-10)
        index: å›¾ç‰‡åºå·
        image_hashes: å“ˆå¸Œå­—å…¸ï¼Œç”¨äºå»é‡
    
    Returns:
        S3å…¬å¼€è®¿é—®URLï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    try:
        # æ£€æŸ¥URLæ˜¯å¦å·²ç»å¤„ç†è¿‡
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()
        if url_hash in image_hashes:
            existing_url = image_hashes[url_hash]
            print(f"ğŸ”„ å›¾ç‰‡URLå·²å¤„ç†è¿‡ï¼Œä½¿ç”¨ç¼“å­˜URL: {existing_url}")
            return existing_url

        # ä¸‹è½½å›¾ç‰‡
        async with session.get(url) as response:
            if response.status != 200:
                print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, URL: {url}")
                return None
            
            image_data = await response.read()
            if not image_data:
                print(f"âŒ ä¸‹è½½çš„å›¾ç‰‡æ•°æ®ä¸ºç©º, URL: {url}")
                return None

            # è®¡ç®—å›¾ç‰‡å†…å®¹å“ˆå¸Œ
            image_hash = hashlib.sha256(image_data).hexdigest()

            # æ£€æŸ¥å†…å®¹å“ˆå¸Œæ˜¯å¦å·²å­˜åœ¨
            if image_hash in image_hashes:
                existing_url = image_hashes[image_hash]
                print(f"ğŸ”„ å›¾ç‰‡å†…å®¹å·²å­˜åœ¨ (å†…å®¹å“ˆå¸Œ: {image_hash[:8]}...), ä½¿ç”¨ç°æœ‰URL: {existing_url}")
                # åŒæ—¶æ›´æ–°URLå“ˆå¸Œè®°å½•
                image_hashes[url_hash] = existing_url
                return existing_url

            # è·å–æ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('Content-Type', '')
            file_ext = get_file_extension_from_url_or_content_type(url, content_type)

            # æ„å»ºS3æ–‡ä»¶è·¯å¾„
            filename = f"{index:03d}{file_ext}"
            s3_file_path = f"images/articles/{date_str}/{filename}"

            # ä¸Šä¼ åˆ°S3
            async with S3ImageUploader() as uploader:
                # å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing_url = await uploader.check_image_exists(s3_file_path)
                if existing_url:
                    print(f"ğŸ”„ S3ä¸­å·²å­˜åœ¨ç›¸åŒè·¯å¾„æ–‡ä»¶: {s3_file_path}")
                    image_hashes[url_hash] = existing_url
                    image_hashes[image_hash] = existing_url
                    return existing_url
                
                # ä¸Šä¼ æ–°æ–‡ä»¶
                public_url = await uploader.upload_image(image_data, s3_file_path, content_type)
                if public_url:
                    print(f"âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ: {s3_file_path} -> {public_url}")
                    # æ›´æ–°å“ˆå¸Œè®°å½•
                    image_hashes[url_hash] = public_url
                    image_hashes[image_hash] = public_url
                    return public_url
                else:
                    print(f"âŒ å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {s3_file_path}")
                    return None

    except Exception as e:
        print(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}, URL: {url}")
        return None 