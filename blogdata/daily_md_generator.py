import os
import json
import hashlib
import shutil
from datetime import datetime, timedelta
import glob
import re
import sys
import pytz

# --- ç¯å¢ƒè‡ªé€‚åº”çš„æ™ºèƒ½è·¯å¾„é…ç½® ---
hugo_project_path = ''
# é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨ GitHub Actions ç¯å¢ƒä¸­
if os.environ.get('GITHUB_ACTIONS') == 'true':
    print("ğŸ¤– åœ¨ GitHub Actions ä¸­è¿è¡Œ, å°†ä½¿ç”¨ç¯å¢ƒå˜é‡ã€‚")
    hugo_project_path = os.getenv('HUGO_PROJECT_PATH')
    if not hugo_project_path:
        print("âŒ é”™è¯¯: åœ¨ GitHub Actions ç¯å¢ƒä¸­, ç¯å¢ƒå˜é‡ HUGO_PROJECT_PATH æœªè®¾ç½®ã€‚")
        sys.exit(1)
else:
    # å¦‚æœä¸åœ¨äº‘ç«¯ï¼Œåˆ™å‡å®šä¸ºæœ¬åœ°ç¯å¢ƒï¼Œè‡ªåŠ¨è®¡ç®—è·¯å¾„
    print("ğŸ’» åœ¨æœ¬åœ°è¿è¡Œ, å°†è‡ªåŠ¨æ£€æµ‹é¡¹ç›®è·¯å¾„ã€‚")
    # __file__ æ˜¯è„šæœ¬è‡ªèº«çš„ç»å¯¹è·¯å¾„
    # os.path.dirname(__file__) æ˜¯è„šæœ¬æ‰€åœ¨çš„ç›®å½• (e.g., /path/to/project/blogdata)
    # os.path.dirname(...) å†ä¸€æ¬¡ï¼Œå°±æ˜¯é¡¹ç›®çš„æ ¹ç›®å½• (e.g., /path/to/project)
    hugo_project_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

print(f"âœ… ä½¿ç”¨ Hugo é¡¹ç›®è·¯å¾„: {hugo_project_path}")

TARGET_TIMEZONE = pytz.timezone("Asia/Shanghai")
print(f"ğŸ•’ ä½¿ç”¨ç›®æ ‡æ—¶åŒº: {TARGET_TIMEZONE}")
# --- è·¯å¾„é…ç½®ç»“æŸ ---

# è‡ªåŠ¨å®šä½ summarized_articles.jsonl çš„æœ€æ–°æ–‡ä»¶
# ä¼˜å…ˆæŸ¥æ‰¾ AI_summary.py ç”Ÿæˆçš„è·¯å¾„
# å…¼å®¹å¤šå¹³å°

def find_latest_summary_jsonl():
    # åœ¨å¤šä»“åº“æ£€å‡ºçš„ Actions ç¯å¢ƒä¸­, è·¯å¾„å¿…é¡»æ˜¯ç¡®å®šçš„
    # å‡è®¾ 'scraper_tool' å’Œ 'hugo_source' åœ¨åŒä¸€ä¸ªå·¥ä½œåŒºæ ¹ç›®å½•ä¸‹
    # å¹¶ä¸” AI_summary.py å·²ç»å°†æ–‡ä»¶ç”Ÿæˆåˆ°äº†æ­£ç¡®çš„ä½ç½®
    # è¿™ä¸ªä½ç½®åº”è¯¥æ˜¯ç”± HUGO_PROJECT_PATH æ¨æ–­å‡ºæ¥çš„
    summary_path = os.path.join(hugo_project_path, 'spiders', 'ai_news', 'summarized_articles.jsonl')
    
    if os.path.exists(summary_path):
        return summary_path
    
    # ä½œä¸ºå¤‡é€‰ï¼Œåœ¨å½“å‰å·¥å…·ç›®å½•é‡Œæ‰¾
    if os.path.exists('summarized_articles.jsonl'):
        return 'summarized_articles.jsonl'
        
    print(f"âš ï¸ è­¦å‘Š: åœ¨é¢„è®¾è·¯å¾„ {summary_path} ä¸­æœªæ‰¾åˆ°æ‘˜è¦æ–‡ä»¶ã€‚")
    return None

# ä»ç¯å¢ƒå˜é‡è¯»å–hugoé¡¹ç›®è·¯å¾„ï¼Œå¦‚æœæœªè®¾ç½®ï¼Œåˆ™è„šæœ¬ä¼šæå‰é€€å‡º
# hugo_project_path = os.getenv('HUGO_PROJECT_PATH') # å·²åœ¨é¡¶éƒ¨å®šä¹‰å’Œæ£€æŸ¥

# ç›®æ ‡æ ¹ç›®å½•
# ä¾‹å¦‚ï¼šC:\Users\kongg\0\content\post
# å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
# è¿™é‡Œå‡è®¾ä¸åŸé€»è¾‘ä¸€è‡´
# ä½ å¯ä»¥æ ¹æ®å®é™…Hugoè·¯å¾„ä¿®æ”¹ target_root
#
target_root = os.path.join(hugo_project_path, 'content', 'post')

# ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
os.makedirs(target_root, exist_ok=True)
print(f"ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨: {target_root}")

def safe_filename(name):
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å¤¹å
    return ''.join(c if c.isalnum() or c in '-_.' else '_' for c in name)[:40]

# è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
def get_content_hash(content):
    # ç§»é™¤å›¾ç‰‡å¼•ç”¨è¡Œï¼ˆå¦‚ ![æ ‡é¢˜](/images/articles/xxx.jpg)ï¼‰
    content_without_images = re.sub(r'!\[.*?\]\(.*?\)\s*\n*', '', content)
    # ç§»é™¤å‰ç½®å’Œå°¾éšç©ºç™½å­—ç¬¦
    content_without_images = content_without_images.strip()
    # è§„èŒƒåŒ–æ¢è¡Œç¬¦
    content_without_images = re.sub(r'\r\n', '\n', content_without_images)
    # ç§»é™¤å¤šä½™ç©ºè¡Œ
    content_without_images = re.sub(r'\n{2,}', '\n\n', content_without_images)
    
    # å¯¹äºæçŸ­çš„å†…å®¹ï¼Œæ·»åŠ ä¸€ä¸ªå‰ç¼€ä»¥é¿å…å“ˆå¸Œç¢°æ’
    if len(content_without_images) < 10:
        content_without_images = f"short_content:{content_without_images}"
    
    return hashlib.md5(content_without_images.encode('utf-8')).hexdigest()

# æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡å¼•ç”¨è¡Œ
def is_image_line(line):
    return bool(re.match(r'!\[.*?\]\(.*?\)', line.strip()))

# è®¡ç®—æ ‡é¢˜çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹ç›¸åŒæ ‡é¢˜çš„æ–‡ç« 
def get_title_hash(title):
    # ç§»é™¤æ‰€æœ‰ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œè½¬ä¸ºå°å†™åè®¡ç®—å“ˆå¸Œå€¼
    normalized_title = ''.join(c.lower() for c in title if c.isalnum())
    return hashlib.md5(normalized_title.encode('utf-8')).hexdigest()

# è·å–å‰ä¸€å¤©çš„æ—¥æœŸç›®å½•
def get_previous_day_folder():
    yesterday = (datetime.now(TARGET_TIMEZONE) - timedelta(days=1)).strftime('%Y-%m-%d')
    yesterday_safe = yesterday.replace('-', '_')
    return os.path.join(target_root, yesterday_safe)

# æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯ï¼ŒåŒ…æ‹¬å†…å®¹å“ˆå¸Œå’Œæ ‡é¢˜å“ˆå¸Œ
def collect_existing_articles_info(days=30):  # å¢åŠ é»˜è®¤å¤©æ•°åˆ°30å¤©
    content_hash_set = set()  # å†…å®¹å“ˆå¸Œé›†åˆ
    title_hash_map = {}       # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„çš„æ˜ å°„
    
    # éå†æœ€è¿‘å‡ å¤©çš„æ–‡ä»¶å¤¹
    for day_offset in range(0, days+1):  # åŒ…æ‹¬ä»Šå¤©(0)
        day_date = (datetime.now(TARGET_TIMEZONE) - timedelta(days=day_offset)).strftime('%Y-%m-%d')
        day_folder = os.path.join(target_root, day_date.replace('-', '_'))
        
        if not os.path.exists(day_folder):
            continue
            
        # æŸ¥æ‰¾æ‰€æœ‰index.mdæ–‡ä»¶
        for root, dirs, files in os.walk(day_folder):
            for file in files:
                if file.lower() == 'index.md':
                    index_path = os.path.join(root, file)
                    try:
                        with open(index_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            
                            # æå–æ ‡é¢˜ - æ”¯æŒå•å¼•å·å’ŒåŒå¼•å·æ ¼å¼
                            title_match = re.search(r"title\s*=\s*['\"]([^'\"]*)['\"]", content) or re.search(r"title:\s*['\"](.*?)['\"]", content)
                            if title_match:
                                title = title_match.group(1)
                                title_hash = get_title_hash(title)
                                # ä¿å­˜æ ‡é¢˜å“ˆå¸Œå’Œå¯¹åº”çš„æ–‡ä»¶å¤¹è·¯å¾„
                                title_hash_map[title_hash] = os.path.dirname(index_path)
                                
                                # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œè®°å½•æ‰¾åˆ°çš„æ ‡é¢˜
                                print(f"ğŸ“ å·²æ”¶é›†æ ‡é¢˜: {title}")
                            
                            # æå–æ­£æ–‡éƒ¨åˆ†ï¼ˆå»é™¤front matterï¼‰å¹¶è¿‡æ»¤å›¾ç‰‡å¼•ç”¨
                            # æ”¯æŒä¸¤ç§æ ¼å¼çš„front matterï¼š+++...+++ å’Œ ---...---
                            content_match = re.search(r'(?:---|\+\+\+)\n.*?(?:---|\+\+\+)\n(.*)', content, re.DOTALL)
                            if content_match:
                                content_body = content_match.group(1)
                                # ç§»é™¤æ‰€æœ‰å›¾ç‰‡å¼•ç”¨è¡Œ
                                filtered_lines = []
                                for line in content_body.split('\n'):
                                    if not is_image_line(line):
                                        filtered_lines.append(line)
                                filtered_content = '\n'.join(filtered_lines).strip()
                                
                                content_hash = get_content_hash(filtered_content)
                                content_hash_set.add(content_hash)
                                
                                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                                print(f"  - å†…å®¹å“ˆå¸Œ: {content_hash[:8]}...")
                    except Exception as e:
                        print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    print(f"å·²æ”¶é›† {len(content_hash_set)} ä¸ªç°æœ‰å†…å®¹å“ˆå¸Œå€¼å’Œ {len(title_hash_map)} ä¸ªæ ‡é¢˜å“ˆå¸Œå€¼ç”¨äºå»é‡")
    
    # è¾“å‡ºéƒ¨åˆ†æ ‡é¢˜å“ˆå¸Œå€¼ç”¨äºè°ƒè¯•
    print("éƒ¨åˆ†æ ‡é¢˜å“ˆå¸Œæ ·æœ¬:")
    sample_count = min(5, len(title_hash_map))
    sample_titles = list(title_hash_map.items())[:sample_count]
    for title_hash, path in sample_titles:
        print(f"  {title_hash[:8]}... -> {os.path.basename(path)}")
    
    return content_hash_set, title_hash_map

# æ£€æŸ¥å½“å¤©æ–‡ä»¶å¤¹ä¸­æ˜¯å¦å­˜åœ¨é‡å¤æ–‡ç« ï¼Œå¦‚æœæœ‰åˆ™åˆ é™¤
def remove_duplicates_in_today_folder(today_folder):
    if not os.path.exists(today_folder):
        return 0
        
    content_hashes = {}  # å†…å®¹å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    title_hashes = {}    # æ ‡é¢˜å“ˆå¸Œ -> æ–‡ä»¶å¤¹è·¯å¾„
    duplicates = []      # è¦åˆ é™¤çš„é‡å¤æ–‡ä»¶å¤¹
    
    # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰æ–‡ç« çš„å“ˆå¸Œå€¼
    for item in os.listdir(today_folder):
        item_path = os.path.join(today_folder, item)
        if os.path.isdir(item_path):
            index_path = os.path.join(item_path, 'index.md')
            if os.path.exists(index_path):
                try:
                    with open(index_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # æå–æ ‡é¢˜ - æ”¯æŒå•å¼•å·å’ŒåŒå¼•å·æ ¼å¼
                        title_match = re.search(r"title\s*=\s*['\"]([^'\"]*)['\"]", content) or re.search(r"title:\s*['\"](.*?)['\"]", content)
                        if title_match:
                            title = title_match.group(1)
                            title_hash = get_title_hash(title)
                            
                            # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
                            if title_hash in title_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤æ ‡é¢˜: {title}")
                                continue
                            title_hashes[title_hash] = item_path
                        
                        # æå–æ­£æ–‡éƒ¨åˆ†ï¼ˆå»é™¤front matterï¼‰
                        content_match = re.search(r'(?:---|\+\+\+)\n.*?(?:---|\+\+\+)\n(.*)', content, re.DOTALL)
                        if content_match:
                            content_body = content_match.group(1)
                            # ç§»é™¤æ‰€æœ‰å›¾ç‰‡å¼•ç”¨è¡Œ
                            filtered_lines = []
                            for line in content_body.split('\n'):
                                if not is_image_line(line):
                                    filtered_lines.append(line)
                            filtered_content = '\n'.join(filtered_lines).strip()
                            
                            content_hash = get_content_hash(filtered_content)
                            
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
                            if content_hash in content_hashes:
                                duplicates.append(item_path)
                                print(f"âš ï¸ å‘ç°é‡å¤å†…å®¹: {title}")
                                continue
                            content_hashes[content_hash] = item_path
                except Exception as e:
                    print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {index_path}: {e}")
    
    # ç¬¬äºŒéï¼šåˆ é™¤é‡å¤çš„æ–‡ä»¶å¤¹
    for dup_path in duplicates:
        try:
            shutil.rmtree(dup_path)
            print(f"ğŸ—‘ï¸ åˆ é™¤é‡å¤æ–‡ä»¶å¤¹: {os.path.basename(dup_path)}")
        except Exception as e:
            print(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥ {dup_path}: {e}")
    
    return len(duplicates)

def get_next_article_index(folder_path):
    if not os.path.exists(folder_path):
        return 1
    
    max_index = 0
    items = os.listdir(folder_path)
    for item in items:
        if os.path.isdir(os.path.join(folder_path, item)):
            match = re.match(r'^(\d+)_', item)
            if match:
                current_index = int(match.group(1))
                if current_index > max_index:
                    max_index = current_index
    return max_index + 1

# æŒä¹…æ€§å“ˆå¸Œå­˜å‚¨çš„è·¯å¾„
HASH_STORE_PATH = os.path.join(hugo_project_path, "spiders", "ai_news", "article_hashes.json")

# åŠ è½½æŒä¹…æ€§å“ˆå¸Œå­˜å‚¨
def load_hash_store():
    if os.path.exists(HASH_STORE_PATH):
        try:
            with open(HASH_STORE_PATH, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å“ˆå¸Œå­˜å‚¨å¤±è´¥: {e}")
            return {"title_hashes": {}, "content_hashes": []}
    else:
        return {"title_hashes": {}, "content_hashes": []}

# ä¿å­˜æŒä¹…æ€§å“ˆå¸Œå­˜å‚¨
def save_hash_store(hash_store):
    try:
        with open(HASH_STORE_PATH, 'w', encoding='utf-8') as f:
            json.dump(hash_store, f, ensure_ascii=False, indent=2)
        print(f"âœ… å“ˆå¸Œå­˜å‚¨å·²æ›´æ–°: {len(hash_store['content_hashes'])} å†…å®¹å“ˆå¸Œ, {len(hash_store['title_hashes'])} æ ‡é¢˜å“ˆå¸Œ")
    except Exception as e:
        print(f"âŒ ä¿å­˜å“ˆå¸Œå­˜å‚¨å¤±è´¥: {e}")

def generate_daily_news_folders():
    today = datetime.now(TARGET_TIMEZONE).strftime('%Y-%m-%d')
    today_safe = today.replace('-', '_')
    today_folder = os.path.join(target_root, today_safe)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(today_folder, exist_ok=True)
    print(f"åˆ›å»ºæ–‡ç« ç›®å½•: {today_folder}")

    # è·å–å½“å¤©æ–‡ç« çš„èµ·å§‹åºå·
    next_article_index = get_next_article_index(today_folder)
    print(f"ä»Šå¤©çš„æ–‡ç« å°†ä»åºå· {next_article_index:02d} å¼€å§‹ã€‚")
    
    # å…ˆæ¸…ç†å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„é‡å¤æ–‡ç« 
    removed_count = remove_duplicates_in_today_folder(today_folder)
    if removed_count > 0:
        print(f"å·²åˆ é™¤å½“å¤©æ–‡ä»¶å¤¹ä¸­çš„ {removed_count} ä¸ªé‡å¤æ–‡ç« ")

    # æ”¶é›†å·²å­˜åœ¨çš„æ–‡ç« ä¿¡æ¯
    existing_content_hashes, existing_title_hash_map = collect_existing_articles_info()
    
    # åŠ è½½æŒä¹…æ€§å“ˆå¸Œå­˜å‚¨ï¼Œåˆå¹¶åˆ°ç°æœ‰å“ˆå¸Œä¸­
    hash_store = load_hash_store()
    for content_hash in hash_store["content_hashes"]:
        existing_content_hashes.add(content_hash)
    for title_hash, path in hash_store["title_hashes"].items():
        if title_hash not in existing_title_hash_map:  # ä¸è¦†ç›–å·²æœ‰è·¯å¾„
            existing_title_hash_map[title_hash] = path
    
    print(f"åˆå¹¶æŒä¹…æ€§å­˜å‚¨å: {len(existing_content_hashes)} å†…å®¹å“ˆå¸Œ, {len(existing_title_hash_map)} æ ‡é¢˜å“ˆå¸Œ")

    summary_jsonl = find_latest_summary_jsonl()
    if not summary_jsonl or not os.path.exists(summary_jsonl):
        print('æœªæ‰¾åˆ° summarized_articles.jsonlï¼Œè¯·å…ˆè¿è¡Œ AI_summary.py')
        return
    print(f"ä½¿ç”¨æ‘˜è¦æ–‡ä»¶: {summary_jsonl}")
    with open(summary_jsonl, 'r', encoding='utf-8') as f:
        articles = [json.loads(line) for line in f if line.strip()]

    # è®°å½•å¤„ç†ç»“æœ
    total_articles = len(articles)
    skipped_articles = 0
    generated_articles = 0
    duplicate_title_count = 0

    # å½“å‰å¤„ç†çš„æ–‡ç« å†…å®¹å“ˆå¸Œé›†åˆï¼Œç”¨äºé˜²æ­¢å½“å¤©å†…é‡å¤
    today_content_hashes = set()
    today_title_hashes = set()

    # æ¯æ¡æ–°é—»ä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼Œå†…æœ‰index.md
    for idx, article in enumerate(articles):
        title = article.get('title', f'news_{idx+1}')
        summary = article.get('summary', '')
        url = article.get('url', '')
        original_content = article.get('original_content', '')
        tags = article.get('tags', []) # ç›´æ¥ä»JSONè·å–tags
        image_path = article.get('image_path') # è·å–å›¾ç‰‡è·¯å¾„
        source = article.get('source', 'æœªçŸ¥æ¥æº') # è·å–æ¥æº
        author = article.get('author', 'æœªçŸ¥ä½œè€…') # è·å–ä½œè€…

        # å¯¹äºæ²¡æœ‰æ‘˜è¦çš„æ–‡ç« ï¼Œç”Ÿæˆä¸€ä¸ªé»˜è®¤æ‘˜è¦
        if not summary:
            summary = "æš‚æ— æ‘˜è¦"

        # æ£€æŸ¥æ ‡é¢˜å’Œå†…å®¹æ˜¯å¦é‡å¤
        original_content = original_content if original_content else ""
        # æ‘˜è¦å’ŒåŸå†…å®¹æ‹¼æ¥ä½œä¸ºå†…å®¹å“ˆå¸Œçš„åŸºç¡€
        content_for_hash = f"**æ‘˜è¦**: {summary}\n\n{original_content}"
        
        # æ‰“å°å®Œæ•´çš„å¾…å“ˆå¸Œå†…å®¹è¿›è¡Œè°ƒè¯•
        content_preview = content_for_hash[:50] + "..." if len(content_for_hash) > 50 else content_for_hash
        print(f"ğŸ“„ å¾…å“ˆå¸Œå†…å®¹é¢„è§ˆ: {content_preview}")
        
        # è®¡ç®—å“ˆå¸Œå€¼
        content_hash = get_content_hash(content_for_hash)
        title_hash = get_title_hash(title)
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” æ£€æŸ¥æ–‡ç« : {title}")
        print(f"  - æ ‡é¢˜å“ˆå¸Œ: {title_hash[:8]}...")
        print(f"  - å†…å®¹å“ˆå¸Œ: {content_hash[:8]}...")
        
        # æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
        if content_hash in existing_content_hashes:
            print(f"â­ï¸ è·³è¿‡é‡å¤å†…å®¹: {title}")
            print(f"  å†…å®¹å“ˆå¸Œ {content_hash[:8]}... å·²å­˜åœ¨")
            skipped_articles += 1
            continue
        if content_hash in today_content_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤å†…å®¹: {title}")
            skipped_articles += 1
            continue
        if title_hash in existing_title_hash_map:
            duplicate_folder = existing_title_hash_map[title_hash]
            print(f"â­ï¸ è·³è¿‡é‡å¤æ ‡é¢˜: {title}")
            print(f"   å·²å­˜åœ¨äº: {duplicate_folder}")
            skipped_articles += 1
            continue
        if title_hash in today_title_hashes:
            print(f"â­ï¸ è·³è¿‡å½“å¤©é‡å¤æ ‡é¢˜: {title}")
            skipped_articles += 1
            continue

        # ç§»é™¤æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        safe_title = re.sub(r'[\\/*?:"<>|]', "", title)

        post_folder = os.path.join(today_folder, f'{next_article_index:02d}_{safe_filename(safe_title)}')
        os.makedirs(post_folder, exist_ok=True)
        print(f"æ­£åœ¨åˆ›å»ºæ–‡ç« : {post_folder}")

        # å‡†å¤‡ Front Matter
        escaped_title = title.replace("'", "''")
        escaped_summary = summary.replace("'", "''")

        # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼Œç¡®ä¿ä¸baseURLå…¼å®¹
        formatted_image_path = ""
        if image_path:
            # ç§»é™¤å¼€å¤´çš„æ–œæ ï¼Œé¿å…ä¸baseURLå†²çª
            if image_path.startswith('/'):
                formatted_image_path = image_path[1:]
            else:
                formatted_image_path = image_path

        md_content = f"""---
title: '{escaped_title}'
date: {today}
tags: {json.dumps(tags, ensure_ascii=False)}
summary: '{escaped_summary}'
image: '{formatted_image_path}'
link: '{url}'
---
"""
        
        # æ­£æ–‡å†…å®¹
        if image_path:
            # åŒæ ·å¤„ç†Markdownä¸­çš„å›¾ç‰‡å¼•ç”¨è·¯å¾„
            md_image_path = formatted_image_path
            md_content += f"![{escaped_title}]({md_image_path})\n\n"
        
        md_content += f"**æ‘˜è¦**: {escaped_summary}\n"

        index_file_path = os.path.join(post_folder, 'index.md')
        with open(index_file_path, 'w', encoding='utf-8') as md_file:
            md_file.write(md_content)
            
        generated_articles += 1
        today_content_hashes.add(content_hash)
        today_title_hashes.add(title_hash)
        next_article_index += 1

    # æ›´æ–°æŒä¹…æ€§å“ˆå¸Œå­˜å‚¨
    new_hash_store = {
        "title_hashes": {},
        "content_hashes": []
    }
    
    # ä¿å­˜å½“å¤©ç”Ÿæˆçš„æ‰€æœ‰å“ˆå¸Œ
    for title_hash in today_title_hashes:
        if title_hash in existing_title_hash_map:
            new_hash_store["title_hashes"][title_hash] = existing_title_hash_map[title_hash]
    
    # æ·»åŠ æ–°ç”Ÿæˆçš„å†…å®¹å“ˆå¸Œ
    new_hash_store["content_hashes"] = list(today_content_hashes)
    
    # åˆå¹¶ç°æœ‰çš„æŒä¹…æ€§å­˜å‚¨
    for title_hash, path in hash_store["title_hashes"].items():
        if title_hash not in new_hash_store["title_hashes"]:
            new_hash_store["title_hashes"][title_hash] = path
    
    for content_hash in hash_store["content_hashes"]:
        if content_hash not in new_hash_store["content_hashes"]:
            new_hash_store["content_hashes"].append(content_hash)
    
    # ä¿å­˜æ›´æ–°åçš„å“ˆå¸Œå­˜å‚¨
    save_hash_store(new_hash_store)

    print("\n--- å¤„ç†ç»“æœ ---")
    print(f"æ€»å…±æ–‡ç« æ•°: {total_articles}")
    print(f"æˆåŠŸç”Ÿæˆ: {generated_articles}")
    print(f"å› é‡å¤è·³è¿‡: {skipped_articles}")
    print("--- --- ---")

if __name__ == '__main__':
    generate_daily_news_folders() 