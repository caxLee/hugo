---
title: 'Attention Sink产生的起点？清华&美团首次揭秘MoE LLM中的超级专家机制'
date: 2025-08-11
tags: ["基模"]
summary: 'MoE LLM中的超级专家机制被首次揭示，对模型性能至关重要。研究发现裁剪超级专家导致模型性能显著下降，尤其在数学推理任务中表现毁灭性。'
image: 'images/articles/2025_08_11/007.png'
link: 'https://www.jiqizhixin.com/articles/2025-08-11-9'
---
![Attention Sink产生的起点？清华&美团首次揭秘MoE LLM中的超级专家机制](images/articles/2025_08_11/007.png)

**摘要**: MoE LLM中的超级专家机制被首次揭示，对模型性能至关重要。研究发现裁剪超级专家导致模型性能显著下降，尤其在数学推理任务中表现毁灭性。
